---
title: "Wastewater Surveillance Interlab Program Analytical Code Documentation"
author: "YJ, JH"
number-sections: true
format:
  html:
    toc: true
    html-math-method: katex

---
```{r,  eval=FALSE, echo = FALSE}
# to-do's
# - set up repository ensure online viewability on interlabs github - JH
# - get comfortable authoring in quarto - both to complete online workshop https://rstudio-conf-2022.github.io/get-started-quarto/
# - insert inventory tables & flow chart images - YH
# - edit tables & images as needed - JH
# - Github section - YH to learn+teach in section
```


```{r, eval=FALSE, echo = FALSE}
#for rendering in other formats
#format:
#  docx:
#    toc: true
#    number-sections: true
#    highlight-style: github

#format:
#  html:
#    toc: true
#    html-math-method: katex
```

```{r setup}
#library(kableExtra)#for dataframe generated tables
library(networkD3) #for flowcharts
library(reticulate)#to run python in this file
```


## Introduction

Within the OCWA interlabs program, R code processes data, and generates plots and descriptive statistics. The R code ("base code") continues to evolve to serve the new requirements and exploratory analysis in the interlabs program. An ancillary collection of spreadsheets with Excel code and python code are used in sample preparation and pre-processing.

The purpose of this document is:
1. To serve as documentation accompanying the base code, and to be continually updated with periodic major review & team refresher training as mandated by the QMS    
2. Set out practice standards for the team to collectively continue hygienic data handling & coding following the refactoring project in 2023; serve as on-boarding training for new team members

# Environment Structure

## External File Structure

```{r}
### I. Code Documentation
#1.  Inventory existing external file dependencies and working procedures outside of main codebase, e.g. the python snippets, address books, flow of sample notes, raw, and clean data *(4 weeks / July 2023)*\
```

An inventory of file and code relationships external to the main codebase is currently is shown in Table 1 and Figure 1 below.

#### Table 1: Spreadsheets Inventory
```{r}
#Yuhan - please load into your excel table as df, then use kable package to format it 
```
: Existing Spreadsheets



## Code Internal Structure
The base code is in the Rmarkdown format with the process and optional analysis codes segmented as chunks.  Explanatory notes are primarily written in markdown prefacing each chunk of code, alongside minor comments in-line within the code chunk.  The chunks should be run from top-down to ensure that prerequisite objects are generated; part of the explanatory notes prefacing each chunk details the prerequisite requirements, and is summarized below.

#### Figure 1: Intra-base code relationships
```{r}
#Yuhan to insert flowchart code and execute the code to show figure
#*Deliverable: Inventory & flowchart of external relationships*
#2.  Inventory existing internal code dependencies and timelines (in-progress work shown in Figure 1) *(4 weeks / July 2023)*\
# **Deliverable: Inventory & flowchart of intra-basecode relationships**
```
: Intra-base code relationships

```{r}
nodes = data.frame("name" = 
                     c("Emails",                                # 0
                       "Raw data",                              # 1
                       "Clean data",                            # 2
                       "20230529_Sample_Notes.xlsm",            # 3
                       "WBE_Sample_Phasma-CA_20230511.Rmd",     # 4
                       "plots",                                 # 5
                       "dataframe",                             # 6
                       "report"))                               # 7

links = as.data.frame(matrix(c(
  0, 1, 1, # Each row represents a link. The first number
  1, 2, 1, # represents the node being conntected from. 
  2, 3, 1, # the second number represents the node connected to.
  2, 4, 1, # The third number is the value of the node
  4, 5, 1,
  4, 6, 1,
  5, 7 ,1,
  6, 7, 1),
  byrow = TRUE, ncol = 3))

names(links) = c("source", "target", "value")
sankeyNetwork(Links = links, Nodes = nodes,
              Source = "source", Target = "target",
              Value = "value", NodeID = "name",
              fontSize= 12, nodeWidth = 30)
```

    

# Operating Environment

All work must be done on OCWA-provided Windows laptops in conformance with public service technology policies.  Overall restrictions applicable to OCWA operating environment will restrict software installation - manager approvals as well as IT "white-list" may be required.  

Working file may be located in the users OCWA laptop local drive, but also available in the Wastewater surveillance team network folder, which is backed-up by IT:   
[//ocwfileeng/eng/Innov and Infra Services/WasteWater Surveillance](//ocwfileeng/eng/Innov and Infra Services/WasteWater Surveillance)

::: .call-out-note
To avoid issues, the whole path to local working project directory and names of script files should not have empty spaces.  (The configuration of the base network directory preceding "WasteWater Surveillance" cannot be changed unfortunately)
:::

## General Software

|Software|Purpose|Min. Version| (Extensions)|
|--------|-------|------------|-----------|
|Notepad ++| version control: pre-merging manual comparison|v8.5| Plugin: Compare v2|
:  Software and Versions used {#software-version}

## R Environment

A reproducible environment for the project inside R is managed using the renv package.  renv() package is used to manage project-local R dependency, with capability to also capture Python dependency (where the latter is run off RStudio using *reticulate*).  Detailed initial set-up [steps](#renv-setup) should be executed by authorized lead only.  All other users can make use of the *renv.lock* file and project library *renv/library* to align with the working R and package versions.  Different project and branches may use different versions of packages without affecting each other.    

### Package Dependencies
Based on line-by-line run of the [base code ](https://github.com/InterlabsOCWA/Base_code/commit/1a4da2e5cbb78f3b97cf4cabb2dd783d574a163e), otherwise named as "WBE_Sample_Phasma-CA_20230607 - facet_fractionplusHK.Rmd", the necessary packages were determined to be (flag jh - maybe take from renv.lock/github main):  
* ggplot2  
* tidyverse
* fs
* janitor
* readxl
* ggh4x
* RColorBrewer
* scales
* writexl
* devEMF
* DescTools
* ggpubr
* officer



::: {.callout-caution}
If your work requires changing versions of or adding packages, please refer to [steps](#renv-setup) steps 2 and 3 as well as consult the team in a timely manner. 
  
Any package update as well as the initial set-up should be done in consultation with the team, as it may force update of certain packages if the versions logged in the controlled lock file are no longer available from the download source.   (https://stackoverflow.com/questions/60779096/error-installing-packages-using-renvrestore)
:::


The following steps are referenced from [NYU's introduction to renv](https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/greene/software/r-packages-with-renv/).  

1) Check your version of R matches the below [flag to update periodically], using the GUI's or by inputting:
```{r}
#| echo: fenced
version
# not as necessary, but the IDE version can be checked using RStudio.Version()
```
2) Make sure you have the renv installed to allow usage of renv commands below.  Version 1.0.0 recommended (this version is also logged in the lockfile)  

3) As part of cloning the project from version controlled branch you are working on, the *renv.lock* (as well as the renv/library, .Rprofile, renv/settings.json, and renv/activate.R files) should be found in your local repository location.   Opening the cloned project at the new location, by executing the below, renv will prepare what is needed for your system to run the project, as long as the same version of R exists between systems:
```{r, eval=FALSE}
## Reproduce environment
renv::restore()
renv::init()
```

4) If package versions in project lockfile do not match the your local library, you can follow the console instruction to generate a list of packages out of sync:  
```{r, eval=FALSE}
renv::status() 
```


::: {#renv-setup .callout-note collapse="true"}
## Set up / Package library updates

The general workflow for the technical lead to initially set up with renv is:  

1. Call renv::init() to initialize a new project-local environment with a private R library.  The console should report back it has linked packages into the project library and written the renv.lock lockfile.  Three new files and directories are created:  

- i. renv/library containing all packages used by the project.  Each project refers to its own isolated library rather than using a common library  
- ii. the lockfile *renv.lock* recording metadata about every package to enable re-installation on other machines  
- iii. the project R profile *.Rprofile* which auto-runs when the project is started in R as a configuration file.  

2. As the project progress, new R packages may be installed, removed or updated.  After validated package changes, call renv::snapshot() to save the state of the project library to the lockfile.  
Option - call renv::restore() to revert to the previous state as encoded in the lockfile if the package change introduced some new problems*

3. A new commit should be made on github so that the team can update their respective local project environment.   These three components must be committed to version control:  
- i) renv.lock 
- ii) .Rprofile
- iii) renv/settings.json 
- iv) renv/activate.R    
A git-ignore file within the renv subdirectory instructs all other non-necessary files **not** to be saved to version control  
:::

For troubleshooting requiring package version check, besides cross-checking renv.lock files, refer to the package window in the corner of RStudio. 

TBD  - Currently referenced from referenced from [NYU's introduction to renv](https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/greene/software/r-packages-with-renv/).  
When renv’s Python integration is active, a couple extra features will activate:  
- renv will instruct reticulate to load your project-local version of Python by default, avoiding some of the challenges with finding and selecting an appropriate version of Python on the system.  
- Calling reticulate::py_install() will install packages into the project’s Python environment by default.  
- When renv::snapshot() is called, your project’s Python library will also be captured into requirements.txt (for virtual environments) / environment.yml (for Conda environments).  
- Similarly, renv::restore() will also attempt to restore your Python environment, as encoded in requirements.txt / environment.yml from a previous snapshot.  

## Python
- To check your version of python, use command line or in python:
```{python}
import sys
print(sys.version)
```


## Postgres SQL database (subject to change)
1. Download the [zip package](https://www.enterprisedb.com/download-postgresql-binaries) from [PostgresSQL](https://www.postgresql.org/download/windows/).  **The installer version requires admin rights, hence use zip**  
2. Unzip in a directory where you have read/write rights.  
3. To allow for command line control - in windows menu, search for "Edit the environment variables" and edit in the Variable "Path"to include the bin directory of where you have unzipped postgres, e.g. "C:\\Users\\hoja\\Downloads\\postgresql-16.0-1-windows-x64-binaries\\pgsql\\bin" 
4. [Create the database](https://www.geeksforgeeks.org/postgresql-installing-postgresql-without-admin-rights-on-windows/) and associate with user.  the user running this command must have full read/write privileges on the directory. they select for this.  First create the folder where you want the database stored, then use its path in the below command.
```sh
initdb -D C:/Users/hoja/AppData/Local/PostgresSQL/pgsql/data -U postgres -E UTF8
```
* -U postgres creates the superuser as postgres
* -E UTF8 will create the database with UTF-8 encoding 

5. To start Postgres server, use the below command adapting the path to where you've previously set as database storage above:
```sh
pg_ctl -D C:/Users/hoja/AppData/Local/PostgresSQL/pgsql/data -l logfile start
```
The command line should then respond with "waiting for server to start.....done.  Server started"  To then access the database using command line, follow step 6.  To access using pgAdmin, follow step 7.

6. Access client using
```sh
psql -U postgres
```sh

To connect to the interlab database you created - 
```sh
postgres#= \c interlab
```

You may encounter the [following error](https://stackoverflow.com/questions/26441873/starting-postgresql-and-pgadmin-in-windows-without-installation) while starting psql.exe:

 "psql: FATAL : role [user] does not exist.

To resolve this,  
i. Be on the same folder path (where you have initdb.exe) i.e. "source-folder/pgsql/bin"
ii. Run the following to login as user "interlab" which has been assigned superuser privileges for the "interlab database".  If not accessing the interlab database, login as general superuser -U postgres.  

```sh
psql -U interlab
```

iii.  if prompted, the password is the same as the user name

Upon successful start up, the command line prompt should change to "database_name=#" as seen at the bottom of next figure, i.e. interlab=# if logging into the interlab database.  SQL commands can be 

iv.
![Top - initial setup, Bottom - routine entering postgres](img/cli-postgres)  
  
To exit, use the “\\q” or “\\quit” commands, and “CTRL + C”, “CTRL + D”, or “CTRL + Z” shortcut keys

7. Locate the "pgAdmin" then "runtime" folder of your unzipped postgres and execute pgAdmin.exe.  Under File > Preferences set up the connection to where your psql bin folder is (per the hint in gray on pgAdmin4)
[](img/pg-pgadmin-connect.png)

8. Add new server - name it as "localhost" in the General tab.  For Connection, also provide "localhost" as the host name.  User, password, and role can all be set to "interlab".

9. In command prompt, enter the following to give user interlab SUPERUSER privileges (on par with user postgres) -> this will allow user to view and edit tables on the pgAdmin GUI.  
```sh
ALTER USER interlab WITH SUPERUSER;
```
To confirm the user has new privileges, enter
```sh
\du
```

10. In pgAdmin, right click on localhost in the Objects Explorer panel to the left of the GUI and click refresh if needed to update.

# Version Control
```{r}
#Yuhan's section
#### II. Training/Establishing Guidelines

#1.  Acquire training on and establish version control on Github *(2 weeks/July 2023)* *Deliverable: Training/Guidance document section, specifically method for & train team as soon as possible on version control/change management**\

### III. Preparatory Version Control & Unit Tests

#1.  Establish version control on Github. All team members to abide by training guidelines when introducing changes to main code base *(2 weeks/July 2023)*\
#    **Deliverable: Version control workflow graphic (to incl. in Training/Guidance document)**\
```

Generic information is provided in this section, with interlabs specific practice at the next section.  If you need to run any of the steps in this section, it is recommended to do so in consultation with the team (e.g. adding a new branch, new repository).  Otherwise you should only need to follow the [project specific version control steps](#vc-project-workflow) for typical work.

## Installation and setup

**1. Install Git from website ** 

Go to [Git website](https://gitforwindows.org/) to download and install it.

**2. Check/update git version**

* Go to the Start menu
* In the Search or Run line type ”cmd” and press enter
* Check git version: `git --version`
* Update git version if have git but not the latest version:`git update-git-for-windows`
    
![git](src/versionUpdate.gif)
    
**3. Introduce yourself to Git**

* Type the following in cmd, replacing that text in quote with your email and name, respectively:  
    `git config --global user.email "you@youremail.com"`  
    `git config --global user.name "Your Name"`
* See result: `git config --global --list`
    
::: {.callout-note}
Note: This **must** be the email associated with your GitHub account.
:::

**4. Install a Git client**
   
"A Git client and an integrated development environment, such as RStudio, are not necessary to use Git or R, respectively. But they make the experience more pleasant because they reduce the amount of command line and provide a richer visual representation of the current state." (from Happy Git with R)

We can use Git client (e.x. [GitKraken](https://www.gitkraken.com/)) to get a nice graphical overview of the recent commit history, branches, and diffs.

![gitkraken](src/gitkrakenScreenshot.png)

*Connect to Git, GitHub and RStudio* 

```{r}
#Comments for Yuhan 
# I would suggest that we don't need to give information on setting up new repository or branches in this doc (besides a general reference link) because we don't want people starting random repository/branches different from what you set up.
# instead can you walk through git clone process which Alex & Chris will need to use?  
# if they git clone do they need to set up connection in RStudio (or command line)?  if not then the current connection step can be moved away alongside making new repositories and branches
# need a brief paragraph on rollback - how to use old commit hash to bring back an older version if a new commit has severe issues
```

**5. Setup a new remote repository in GitHub**  
A new repository may be set up for discrete projects.  (For the most part, we will be working in the base code repository at https://github.com/InterlabsOCWA/base_code)
    ![repo](src/SetRepo.gif)

**6. Create branches**

To work on different versions of the base code at one time, we need to branch off the specific branch we want to work on. It will copy all the files in the branch at that point in time.

::: {.callout-important}
Remember to select which branch you want to branch off **AND** Pull it to your local after you create branch !!
:::
    
There are two ways to create a new branch in both local and remote:
   
  - From GitHub:
   
    ![gitbranch](src/GitHubBranch.gif)
   
   - From Rstudio:
     
     Switch to the branch you want to branch off ([see switch branch section in ](#branch_workflow)in branch workflow) then create branch followed by below image.
      
     ![Rbranch](src/RBranch.png)
     
::: {.callout-note}
For Rstudio method, you need to finish **step 3** below first.
:::

**7. Connect Rstudio to GitHub and clone the repo to local**
![connect](src/RStudioConnectGitHub.gif)
  
After click “Create Project” to create a new directory, which will be all of these things:
  
- a folder with same remote repository name on your computer
- a .gitignore file
- an RStudio Project
- any files you upload into remote rpository

**8. Detect Git from RStudio**

* Type `where git` in cmd to get the git executable path
* From RStudio, go to *Tools > Global Options > Git/SVN* and make sure that the box *Git executable* points to your Git executable.
    ![detect](src/detect.png)

## Fundamentals branch workflow {#branch_workflow}

**Overall flow chart**

The image below shows a overall git flow chart.
![flow](src/flow.png)

**1. Switch to corresponding branch**

Since different people can be working on different branches, before starting our work, we need to switch to our [working branch](#vc-project-workflow). There are two methods to switch between branch:

* In Rstudio:

    Select the remote branch you want to work on by clicking the bottom (see image below), it will automatically create and switch to that local branch.
  
    ![Rbranch](src/SwitchBranch.png)
     
* In Shell: type `$ git checkout <branch_name>` in terminal

**2. Pull from remote**

To start working on a project, pull any new changes to update local copy. If there is a merge conflict see [merge_conflict section](#merge_conflict).

* In Rstudio: Click the **Pull** bottom in the **Git** tab section

* In Shell: type `$ git pull` in terminal

**3. Make changes and commit**

* After finishing changes, click on the **Git** tab in RStudio. It will show the files that have changed or added.

![git](src/git.png)

* Click on the **Commit**, it will show the window in below image. Click the staged box to stage the file you want to push to remote. You can see the difference between the files after you click the file (show blue line). Lines that have been added are green. Lines that have been removed are red.

![diff](src/diff.png)

* Add a commit message and click on **Commit** in this window. Every time make a commit, must also write a short **commit message**.
    
* You can view the history of your commit after click the left corner **History** beside the change. You can also view the commit history in other Git client app. Or check your past commits though type `git log` in terminal.

    
* Check on the state of the Git world: type "git status" in terminal

::: {.callout-note}
For the files you don't want to push to Github you can list the them in [**.gitignore** file](#gitignore).
::: 

**4. Push**

When you are finished and committed, remember to **Push** these changes to the remote repository (e.g., GitHub). Should see 'branch -> branch' if successfully pushed.

::: {.callout-note}
You might need to pull again before push, if someone pushed their work to remote during your work.
::: 
   
#### Pull Request and Merge Changes between branches {#pullrequest}

Each time you want to merge/pull a branch from another branch, you will need to create a pull request. Sometimes it will show up automatically in the branch web page after you push, if not you need to do it manually.

* For the first auto case, you will see a notice to create a pull request (see the image below). Then click on the button that says **Compare and pull request** to go next step. 
![pull_auto](src/pullrequest.png)
* For the manual case, first need to click the **Pull request** tab. Then click the button that says **New pull request**. It will show up the **Compare changes** page, you need to decide the pull direction. The compare branch is the one you want to pull from. After decide the branches, click **Create pull request** button and go next step.

* Now both case are in the same **Open a pull request** page, and click on **Create pull request** to compare and merge them.

           
* GitHub has now compared the two versions of the project that are in the two branches. If there is no conflict it is telling us that there are no conflicts between the two versions and you can click the green **Merge pull request** button to merge the changes.

* If you don't need the branch (e.x. feature branch), click **Confirm merge**. Go ahead and delete the branch, since its changes have been incorporated, with the **Delete branch** button in the purple box.
![success](src/successMerge.png)

The manual pull request process: ![manual](src/manualpullrequest.gif)

#### Merge conflict {#merge_conflict}
There will be a merge conflict when you pull from the remote but have done some new work locally since the last time you pulled:

* Recent commit history of origin/main: A--B--C
    
* Recent commit history of the local main branch: A--B--D

It will show the warning below and automatically add the two version with markers in that file.
    
    'CONFLICT (content): Merge conflict in ...(the file have conflict)
    Automatic merge failed; fix conflicts and then commit the result.'      
      
* The code between the <<<<<<< HEAD and ======= line is the content you committed in your local and the code between the ======= and >>>>>>> line is the content in the current remote branch.

![marker](src/mergeconflict.png)

* Keep the version you want and remove the all the markers line (<<<<<<< HEAD:, ======, >>>>>>) inserted to demarcate the conflicts then save the file and do the following commit and push again.

### gitignore file {#gitignore}

The .gitignore file can tell Git which files and directories to ignore when you make a commit. You can commit and push the .gitinore file to share the ignore rules with other users.

The .gitigonre content writing reference: <https://swcarpentry.github.io/git-novice/06-ignore.html>
   
The data/files we don't want to commit to GitHub:

- all the data spread sheet
- plots
- reports

## Version control project workflow {#vc-project-workflow}
```{r}
#Comments for Yuhan 
#- need the graphic cleaned up so it makes sense to a regular reader like Alex - e.g. what does it mean CA will mainly work here?  next round could be you or I, need the description for the task not a person
# what is JIRA-35 example feature?  can you change this example to match something we might see in interlabs.  also use our naming convention which applies to branches as well (so the double dash and space are not acceptable)
# bottom comment - "feel free [...]" can you add the note about how the feature or develop branch gets merged?  It's a note to yourself (I think) so don't leave it for the reader)

#TBD
# explain bare min. information you need with commit message like you started with the annotation under the round dots.  Initial name is probably the least important info in the commit since git would track who pushed the change anyways?  not sure
# explain how each dot is a commit like a save point we can revert to by knowing the commit hash e.g. commit ba9da59 
# I moved the home folder description up right beneath the diagram.  We need a description of the normal working "place" (the git clone local - you mentioned in the general set up for setting up a new repository but can you add description of how it would be like for our project base code?  especially with branches - does the user need to git clone each branch into a separate folder, or can they switch after git cloning the main?  this needs to be very clear assuming the reader is a new student , for example).  How often are we to sync/commit to github from local?  Then, back to the network folder home locations - how often are these to be "synced" with the github versions?  Decide on a frequency and that will become part of the QMS requirement for data management.

```


**The overall workflow graph of version control**

The diagram below shows the overall version control workflow for each round.

![workflow](src/workflow.png)

The corresponding locations for the branches in the network drive are as follows (Flag: Need to confirm).

- The home folder locations for **main** branch:

    //ocwfileeng/eng/Innov and Infra Services/WasteWater Surveillance/3_Templates_References/Round_codes
   
- The home folder locations for **Round** branch: 
   
    //ocwfileeng/eng/Innov and Infra Services/WasteWater Surveillance/4_WIP/O103-Interlabs/d_Code
   
- The home folder locations for **develop** branch:
   
    //ocwfileeng/eng/Innov and Infra Services/WasteWater Surveillance/1_QMS


### Main branch
The main branch contains the base code template that will be updated during each round, so we need to upload/ push the initial files at beginning. A workshop will be hold to compare the files using [Notepad++](#notepad) and decide the merge request before merging the subbranch to main.

![Cross-checking in Notepad++ prior merge](img/premergeDiffNotepad.png){#notepad}

::: {.callout-warning}
People should not switch to main branch and make changes to it during work.
::: 

### Round branch
The round branch contains current round code, it will be update during each round.

The workflow for round branch: 

1. Create manual [pull request](#pullrequest) to update the round branch from main in GitHub.
2. Do the [branch workflow](#branch_workflow) steps.
3. After finished this round code, create pull request to merge to main branch.

Q: need feature branch for this?

::: {.callout-warning}
For step 3, only need to create pull request. Do not need to merge it!
::: 

### Develop branch
The code refactoring work will in this branch.

The workflow for round branch: 

1. Create manual [pull request](#pullrequest) to update the round branch from main in GitHUb.
2. Do the [branch workflow](#branch_workflow) steps.
3. Might need to create feature branch to add new feature to the base code.
4. After finished this round code, create pull request to merge to main branch.

::: {.callout-warning}
For step 4, only need to create pull request. Do not need to merge it!
::: 


   
#### Unsolved questions
- Zombie code: cut off but might come back?

# Issues Tracking
For most cases, the project issues tab in GitHub should be used to log issues (including bugs and features suggestions) and assign staff to resolve, using this [workflow](https://docs.github.com/en/issues/tracking-your-work-with-issues/quickstart) within repositories set up as projects.  

Issues are repository wide (i.e. cannot be "created" specific to a branch). To aid tracking, the title of the issue should state the specific branch it is for. Additionally, it is encouraged to add labels to help identify type of issue and the branch it is for.  A partial exception is features request which, after being created as an issue in the appropriate parent branch, should be made its own branch to differentiate it even more.  

![Issues creation and syntax](img/githubIssuesCreate.png)

1. [Creating an issue](https://github.com/InterlabsOCWA/base_code/issues)  
The required syntax for issues title is as follows:  
Title: *Branch*-*Type*_*Descriptor*  
Where Branch = *branch name* e.g. should primarily develop unless you are reporting a bug on the round branch
Where Type = feature OR bug OR refactor, for example
Where Descriptor = short human-readable descriptor of request

2. Adding details 
(click on the gear icons on the right panel of the github issue page to add the following)  
* *Label* appropriately by branch and any other help tags at the right panel
* Assign *Project* (usually the "@InterlabsOCWA's BaseCode") at the right panel
* Assign *Assignee* as the lead staff responsible
* Provide sufficient context in the *comment* box in case other staff need to follow up on the issue
* if this is a new feature, once the issue is submitted and there is to resource to tackle the feature addition, the user should (reload the issue page) create a new branch for it under the development section, right panel.  The branch should be a branch off from the develop branch, as illustrated below.     
![Issues creation and syntax](img/githubIssuesBranch.gif)

3. Update as appropriate, with commit hash if it is tied to a specific upload version.  By placing a commit hash, github will automatically provide a link to that committed version.  

4. Resolve and close.  
* Bugs: When resolved, staff should closed issue with sufficiently detailed comment for potential future reference, and include the git commit hash of the version pre/post-fix if appropriate.  Bugs should be resolved at the develop or round branches; for merges into the main branch, consult the team or wait until the milestone merges.
* Features request & refactoring tasks: these changes are to be reviewed by staff other than the coder to validate functionality & impacts.  Reviewer should close the issue once that is confirmed.  The git commit hash should be logged in the closure comment. 




# Linting / Code Style Guide
*General*
The [tidyverse style](https://style.tidyverse.org/) will be the default style, and aligns with the lintr package used below.  It is recommended for all users to review the guide.  Some general format rules include:  
* camelCase recommended (note tidyverse style guide prefers snake case but both are acceptable)
* tab indentation by two spaces
*Linting - continuous integration*
In RStudio IDE:  
*-* some linting is included, however it is for errors only
*-* selecting code and hitting Ctrl + I will re-indent code for you
  
*lintr*:  
lintr package will be set up with the main code to provide diagnostic linting (i.e. as opposed to auto-correction using stylr package), recommending but not enforcing stylistic changes.  It will activate with github upload workflows following this set up command, which also creates a **.github/workflows** folder with **lint.yaml**  
  
```{r}
#| eval: FALSE
usethis::use_github_action("lint")
# for context, check in console - vignette("continuous-integration")

```

Once lintr is installed, it can be accessed in RStudio.  In order to see the diagnostic results, first enable the "Markers" pane by  > “Tools” > “Global Options…” > go to “Code” on the left-hand-side > “Diagnostics” tab > check “Show diagnostics for R”.  The Markers pane will then be available tabbed next to Console/Terminal/Background Jobs to display lint results.  

To run lintr, open Tools > Addins. From the pop-up window, select lintr from the addin's list and execute on the open code file.  
![](img/lintrRStudio.png)

::: {.callout-note} You can bind this linting process to a keyboard shortcut in RStudio in the addin's window, with Shift+Alt+L recommended *
The user may then use the lintr result in the Markers pane as a guide to modify code format.  Automated linting (e.g. styler package) is not recommended at this point.  More options for linting and details are available for reference at https://blog.r-hub.io/2022/03/21/code-style/ 
:::

# Refactoring 
- recommend every x period
- workflow

## unit testing approaches & practices 
*At the minimum, we should aim to write tests for the "happy path" of our application.

What is a "happy path" test? "Happy path" tests are very useful, because they catch the most critical bugs, but are definitely not sufficient for building reliable and robust applications.* https://effectivecio.com/2009/11/02/the-happy-path/

The happy path is the path through a system where everything works, the data is correct, the system stays up, and the users are well-behaved.  We tend to test the happy path first because we understand how the system should function and want to ensure that the basic features should work.  We need to move off the happy path and wander in the weeds.  What if the quantity exceeds stock on hand? What if the user enters too few digits from their credit card? Or too many? Or adds a space, or a dash? What if the zip code doesn’t match the state? What if? What if? What if?

It doesn’t take long to test the happy path.  It takes forever to test everything off the happy path. You need to spend a lot of time wandering away from the happy path, and maybe there is a reverse rule for testing: 20% of your time on the happy path; 80% of your time off of it.

![ What are Unit Testing, Integration Testing and Functional Testing?](http://codeutopia.net/blog/2015/04/11/what-are-unit-testing-integration-testing-and-functional-testing/)

Automated testing is the practice of writing code to programmatically test the actual code we want to write.

It offers several benefits over manual testing:

it saves testing time (by not having to perform manual tests over and over)
it saves debugging time (by catching bugs earlier)
it makes it easier to program (because we don't need to keep the entire application in our heads, just the part that we're working on... if we break something, our tests will let us know)
it makes it easier to come back to a program after some time (programmers forget things, but tests do not)
it makes it easier to work together (we wrote some widget and know how it works, but our team-mates probably don't; our tests will catch bugs introduced by others on our team, and vice versa)
it acts as documentation (readings tests is a great way to learn about how code is meant to be used)
it improves the quality of our code (writing code that is easy to test often requires us to change how our code is structured -- for the better)
##  Issues-tracking method


7.  Document the procedures for above practices and create training presentations for the team *(3 weeks)*\
    **Deliverable: Training/Guidance document**\
    **Deliverable: Training session**

## Unit Testing Framework
2.  Set up formal testing, enabling validation of no-loss-of-function and establishing built-in "bases" during refactoring. *(August 2023)* **Deliverable: Test framework & test-suite code**

Cursory review in June 2023 found that base code functions (more suitable for testing) are few and generally used to generate plots (unsuitable for testing). Testing may be adapted to instead inspect dataframe size & output value spot-check.

-------------------------------

1.  Packages/Libraries organization *(3 weeks / July 2023)*:
    1.  create reference document of package purpose/usage in the program (including any removed but potentially useful)
    2.  remove any currently unused packages, narrowing list/call to just the necessary ones
    3.  create packrat compilation or Docker of used packages to preserve running versions\
        **Metric: Number of superfluous packages/libraries, number TBD to 0**\
        **Deliverable: One packrat / docker with all in-use packages including versions**
2.  Linting for consistent style *(1 week; August 2023)*\
    **Deliverable: Commit \[track number\]**\
3.  Refactoring cycle, which will be repeated for each issue/potential improvment. As adapted from Fowler: 0.1 Select issue/potential improvement to work on. Create branch on version control 0.2 Introduce comments/pseudo-code for changes. Obtain consensus with *reviewer*  
**Deliverable: Branch & Commit \[track number\]**
    1.  Create getters and setters for the field\
    2.  Locate all references: replace access with calls to the getter, and changes to the field with calls to the setter\
    3.  Compile and test after changing each reference\
    4.  Declare the field as private\
    5.  Compile and test. Switch to light mode for coder & reviewer\
    6.  *Reviewer* checks pull request, code and merge. Light mode for all users\
        **Deliverable: Commit \[track number\]**
4.  Documentation updates  
**Deliverable: Code & Guidance Documentation Updates, Inventory of updated intra/external code relationships **


### Table 3 - Milestone Deliverables & Tentative Schedule
| Deliverable                                                     | Timeline [In Progress/Completed] | Resp./Review |
| --------------------------------------------------------------- | -------------------------------- | ------- |
| Code Documentation                                              |                                  |         |
| 1.Inventory & flowchart of external relationships               | Summer 2023 / July 2023          |         |
| 2.Inventory & flowchart of intra-basecode relationships         | Summer 2023 / July 2023          |         |
| 3.Packages/Libraries Listing                                    | Summer 2023 / July 2023          |         |
| 4.Code Documentation - existing code comments archival          | Summer 2023 / July 2023          |         |
| 5.Kaban battle board (initial version)                          | Summer 2023 / July 2023          |         |
| Training/Guidelines                                             |                                  |         |
| 6a.Guidance Documentation - Version Control & Issues resolution | Summer 2023 / July 2023          |         |
| 6b.Guidance Documentation - Refactoring & Unit Testing          | Summer 2023 / August 2023        |         |
| 6c.Guidance Documentation - Linting, Selected Style             | Summer 2023 / July 2023          |         |
| Preparatory Staging                                             |                                  |         |
| 7.Version control established, team training session            | August 2023                      |         |
| 8.Testing framework                                             | August / September 2023          |         |
| Refactoring                                                     |                                  |         |
| 9.Refactoring cycles                                            | Autumn 2023 / November 2023?     |         |
| 10a.Code & Guidance Documentation Updates                       | Autumn 2023 / November 2023?     |         |
| 10b.Inventory of updated intra/external relationships, packages | Ongoing / December 2023          |         |
| 11.Training sessions, monthly                                   | Ongoing                          |         |
| 12.Project Completion Memo, performance metric                  | January 2023                     |         |
|                                                                 |                                  |         |

# Interactive QAQC App
An upstream QAQC workflow was duplicated and separated out from the base code, as the process can lead to requests-to-amend or rejection of lab submittals (and consequently a hard "stop" to running the base code so the problematic submittal can be temporarily excluded).

The QAQC workflow aligns with the QMS SOP and the participants ToR into a Shiny app that allows the analyst to singly screen each submittal with visual aid.  Submittals are also optionally collated into a PostgresSQL database for future use.

## Usage Scenarios
The version control for this application is in https://github.com/InterlabsOCWA/std_curve   
Development occurs in two branches, the main differentiator being the presence of the database.  
### web branch
for quick usage, has "debug" file (which loads in a specific Rey round filled template for testing)
### db branch

## Future Framework Migration Research & Reflect
The future intention is for the interlabs QAQC app to be available to participating labs, thereby separating potential role conflicts for the screening interlab staff (who should not be correcting submittals).  This utility app may be need to be built in tandem the main interlab website.  

A September 2023 successful Shiny-based regulatory [submission](https://pharmaverse.github.io/blog/posts/2023-08-14_rhino_submission_2/rhino_submission_2.html) to the FDA can be considered a litmus test of recognition for the use of Shiny in compliance work requiring statistical review and evaluation. 

The current QAQC app is built with flexdashboard with Shiny to allow for reactive elements.  The advantage is there is no separated ui and server files, allowing time to be focused on building functionality rather than building a shiny app.  However, in consideration of expansion with other website functions (e.g. user authentication, payment, multi-page navigation), it is unlikely the flexdashboard can support future needs.  

Furthermore, to support the expansion, future staff is expected to have a full-stack dev background - hence the code needs to move towards more formal organization in alignment with typical full stack practices.  A pre-packed framework rather than our current dashboard/configuration using individually curated packages may be more conducive for maintenance.

### Available Framework options
The most prominent R Shiny packages for providing a opiniated framework for full stack Shiny apps are Rhino, Golem, and Leprechaun (as reviewed by [R-bloggers](https://appsilon.com/rhino-vs-golem-vs-leprechaun/)).  

Leprechaun is not further evaluated as it does not have a built-in testing framework, which is desirable for our use.  However, it does not add extraneous (and itself) as dependency, which is advantages for the size and fragility of applications.  It may be worthwhile exploring at a future date.
![available frameworks](https://wordpress.appsilon.com/wp-content/uploads/2023/11/Rhino_v_Golem_V_Lepre.webp)

Rhino is the "new kid on the block", having been released by Appsilon early 2022. It is underpinned by the Box package.  Golem, released by ThinkR, is the more established framework, running under 

Both Rhino and Golem are accompanied by their respective "universe" of supporting packages which facilitate shiny use for production, such as React and Docker enablement, state management, benchmarking.  The availability of these tools would be very helpful for the development of the interlabs app and hence, these two frameworks were evaluated further.

#### At-first-glance
 
**Folder structure** : Both frameworks emphasize modularity in construction.  Rhino is reminiscent of a JS or Python flask framework with separate views, styles, testing directories.  Golem is reminiscent of R package development, being more simply separated essentially by workflow or roadmap within the key dev folder: configuration, development and deployment file-creation R files.  A further R directory contains its UI, server and modularized logic operations R files.
The folder structure organization are follows, respectively for the Rhino and Golem.

```
Rhinoappy
├───.github
│   └───workflows
├───app
│   ├───js
│   ├───logic
│   ├───static
│   ├───styles
│   └───view
├───renv
│   ├───library
│   │   └───R-4.2
│   │       └───x86_64-w64-mingw32
│   │           ├───(various packages)
│   │                   └───files
│   └───staging
└───tests
    ├───cypress
    │   └───integration
    └───testthat
```    
    
```
golemappy
├───dev
├───inst
│   └───app
│       └───www
├───man
└───R
```

**Dependency management** : The Rhino framework uses renv (the modern version of packrat) to control versions of package dependencies used.  Golem (as learnt in the pharmaverse workshop) may also use renv, although that was not clear in its template set-up (perhaps requires an extra call?).  

**Learning Curve & guidance** : Both frameworks have good documentation.  However (as described in detail in next section's time-boxed trials), golem has very thorough and walkthrough-like comment-prompts in the script templates, nearly adequate to fully code-along within the script.  Whereas, Rhino provides no in-script guidance, just a basic example (similar to npx create-react-app).  It is necessary to working with the docs open by its side.  Just by the sheer extent of the default template folder structure, it takes more time to figure out and update all the corresponding elements.  Its similarity to other full stack frameworks will be conducive to navigation and more accessible to those with a software engineering perspective.

**How to render (in developement** 
Rhino is simpler in this aspect, requiring a single console command.  Golem (if any changes had been made) requires several more lines to be run, but that is all provided in the run_dev.R file.

Rhino
```
shiny::runApp()
```

Golem
```
# Detach all loaded packages and clean your environment
golem::detach_all_attached()
# rm(list=ls(all.names = TRUE))

# Document and reload your package
golem::document_and_reload()
golem::run_dev()
```

**Common concepts** namespace, modularization



![Rhino, exclusive of its E2E tests and config settings, as conceptually broken down by Jinhwan Kim](https://jhk0530.medium.com/how-to-make-cheatsheet-for-r-package-rhino-5cdfa22c308e)


#### Rhino vs. Golem
In discussion of these frameworks with a mentor (not fluent in R), a take-away advice was to check the number of downloads - e.g. in the frameworks the mentor would advise for python (flask), downloads would be in the millions.

A roughly^ 1-hr time-boxed trial was conducted in both frameworks to "wet the toes".  The objective was to create an app with two plots, using a native dataset (easy proof-of-concept), then using the CRAN download counts for the two packages (see how to add a second modular feature)

**Tabulate overall impressions**
https://docs.google.com/document/d/1MotX9AKWBvvwUBoE2rd5J92uZmP4x0OFo0V1nDq4nQo/edit

# References


*WWS Working Platforms*\
https://github.com/InterlabsOCWA\
https://www.goodday.work/p/GKN7K9

*Inventory/Code Profiling* https://bookdown.org/csgillespie/efficientR/performance.html#performance-profvis\
https://github.com/lewinfox/foodwebr https://www.researchgate.net/publication/305738639_A\_Proposal_of_Refactoring_Method_for_Existing_Program_Using_Code_Clone_Detection_and_Impact_Analysis_Method
https://rstudio.github.io/packrat/walkthrough.html  

*Unit Testing*\
https://r-pkgs.org/testing-design.html

https://webcache.googleusercontent.com/search?q=cache:q9QbZVfMwtQJ:https://www.r-bloggers.com/2019/11/automated-testing-with-testthat-in-practice/&cd=14&hl=en&ct=clnk&gl=ca

*Refactoring*\
Lemaire, 2020. *Refactoring at Scale*\
https://learning.oreilly.com/library/view/refactoring-workbook/0321109295/

*Version Control* (add existing RStudio project to Github): https://happygitwithr.com/index.html

https://hansenjohnson.org/post/sync-github-repository-with-existing-r-project/

https://aberdeenstudygroup.github.io/studyGroup/lessons/SG-T1-GitHubVersionControl/VersionControl/

#2.4.4.

# Appendix/Reference Extracts
## Dark Mode/Light Mode Technique

(extract from Lemaire, 2020. *Refactoring at Scale*)\
Dark Mode / Light Mode : We can compare pre-refactor and post-refactor behavior by employing what we've coined at Slack as the light/dark technique\
Dark mode : Both implementations are called\
The results are compared\
The results from the old implementation are returned

Light mode :\
Both implementations are called\
The results are compared\
The results from the new implementation are returned

How To ?\
Once the abstraction has been properly put in place\
Start enabling dark mode\
Monitor any differences being logged between the two result sets\
Track down and fix any potential bugs in the new implementation causing those discrepancies\
Repeat this process until you've properly handled all discrepancies, Enabling dark mode to broader groups of users

Once all users have been opted in to dark mode\
Continue logging any differences in the result sets\
Continue to opt broader groups of users into light mode, until everyone is successfully processing results from the new implementation.

Disable execution of both code paths\
Remove the old logic altogether : only the new implementation should remain

## Typical Code Issues & Correspondent Refactoring

### Refactoring methods

a.  Extraction: breaking code into chunks or utilizing existing chunks for isolation into separate functions and replaced with a call to the functions. Change patterns into abstraction. Similarly for variables
b.  Inline refactoring: finding functions calls and replacing them with the function content (opposite of extraction)
c.  Simplifying methods logic: consolidate conditional fragments and using polymorphism instead of conditionals
d.  Simplifying method calls: examining parameters involved and adding/replacing as needed

## Handling Commented-out code
\## stuff to incorp. COMMENTED-OUT CODE In the case of commented-out code, it's pretty obvious that the code is unused. I always recommend that developers who are tempted to comment-out code instead simply delete it if the code is tracked using version control. If you need it again someday, you can easily recover it by going back through your commit history.

new function to help mitigate code degradation. Let's say we write up a simple helper to encapsulate all the logic for validating a user object; we'll call it validateUser

A NOTE ABOUT COUNTING LINES OF COMMENTS By and large, ignoring comments is good practice when counting lines of code. Docblocks and inlined TODOs do not affect the behavior of our programs, so including them in our size calculations would not help us better characterize a program's complexity. In practice, however, I've noticed that you can easily pinpoint some rather perplexing sections of code by counting the number of inline comments at the function level. In general, developers tend to leave inline code comments when the surrounding logic is difficult to follow. Whether that's because the code is dealing with a complicated piece of business logic or it has just become gradually more convoluted over time, we tend to leave some pointers to others who come after us when modifying exceptionally tricky code. Therefore, we can use the quantity of inline comments within a single function, whether short or long, as a possible warning sign.

When we're working to improve an existing implementation, whatever the extent of our endeavor, we want to be sure that we're correctly retaining its behavior. We can safely assert that our new solution continues to work identically to the old by relying on the original implementation's test suite. Because we are relying on the test coverage to warn us about potential regressions, we need to verify two things before beginning our refactoring effort: first, confirm that the original implementation has test coverage and, second, determine whether that test coverage is adequate.\
refactoring requires us to be able to ensure that behavior remains identical at every iteration. We can increase our confidence that nothing has changed by writing a suite of tests (unit, integration, end to end), and we should not seriously consider moving forward with any refactoring effort until we've established sufficient test coverage.

## Unit testing notes

-   Set a canary test (it checks if the test framework works)

-   Add few (automated) tests. Start with code candidate to change first. If code wasn't designed to be testable, don't change it yet. Make your best to test it as is. This is very important.




### Misc. heap of code
Find main package list + versions:
```{r}
#| echo: fenced
print(head
      (tibble::tibble(
        Package = names(installed.packages()[,3]),
        Version = unname(installed.packages()[,3])
      ), n=600 #where n = an appropriately large value to capture all your packages
      )
)
```

```sh
cmd //c tree
```
